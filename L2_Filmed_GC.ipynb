{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Overview of Automated Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API tokens for our 3rd party APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.auth\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/mfunaki/.config/gcloud/application_default_credentials.json\"\n",
    "credentials, project_id = google.auth.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_circle_api_key\n",
    "cci_api_key = get_circle_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_gh_api_key\n",
    "gh_api_key = get_gh_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_google_api_key\n",
    "google_api_key = get_google_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our github branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mfunaki-circleci/llmops-course1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_repo_name\n",
    "course_repo = get_repo_name()\n",
    "course_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dl-cci-affirmative-pillow-49'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_branch\n",
    "course_branch = get_branch()\n",
    "course_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sample application: AI-powered quiz generator\n",
    "We are going to build a AI powered quiz generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset for the quizz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template  = \"{question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n  \\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category. \\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to observe the content or generated object\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatVertexAI(project='plucky-agent-412507', model_name='gemini-pro', client=<vertexai.preview.generative_models._PreviewGenerativeModel object at 0x13f063f40>, convert_system_message_to_human=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "llm = ChatVertexAI(project='plucky-agent-412507', model_name=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an output parser in LangChain that converts the llm response into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the pieces using the pipe operator from Langchain Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n  \\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category. \\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])\n",
       "| ChatVertexAI(project='plucky-agent-412507', model_name='gemini-pro', client=<vertexai.preview.generative_models._PreviewGenerativeModel object at 0x13f063f40>, convert_system_message_to_human=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the function 'assistance_chain' to put together all steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking all components and making reusable as one piece\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "  \n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"user\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'eval_expected_words' for the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser)\n",
    "    \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "    \n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Generate a quiz about science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"user\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### Category: Science\n",
      "\n",
      "Step 2:#### Selected Subjects:\n",
      "1. Telescopes\n",
      "2. Physics\n",
      "\n",
      "Step 3:#### Quiz:\n",
      "\n",
      "Question 1:#### What is the name of the largest telescope in space?\n",
      "A. Hubble Space Telescope\n",
      "B. Chandra X-ray Observatory\n",
      "C. James Webb Space Telescope\n",
      "D. Spitzer Space Telescope\n",
      "\n",
      "Question 2:#### The first refracting telescopes were invented in which country and during which century?\n",
      "A. France, 19th\n",
      "B. Italy, 16th\n",
      "C. Netherlands, 17th\n",
      "D. England, 18th\n",
      "\n",
      "Question 3:#### What is the reason why the Eiffel Tower in Paris is taller in the summer than in the winter?\n",
      "A. Snow accumulation\n",
      "B. Wind currents\n",
      "C. Expansion of the metal\n",
      "D.  Leaning of the tower\n"
     ]
    }
   ],
   "source": [
    "eval_expected_words(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    expected_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'evaluate_refusal' to define a failing test case where the app should decline to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new question (which should be a bad request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about Rome.\"\n",
    "decline_response = \"I'm sorry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the refusal eval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:pink; padding:15px;\"> <b>Note:</b> The following function call will throw an exception.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the category for the quiz. Please choose from the following: Geography, Science, or Art.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the bot to decline with     'I'm sorry' got Specify the category for the quiz. Please choose from the following: Geography, Science, or Art.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_refusal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecline_response\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mevaluate_refusal\u001b[0;34m(system_message, question, decline_response, human_template, llm, output_parser)\u001b[0m\n\u001b[1;32m     16\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m decline_response\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mlower(), \\\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the bot to decline with \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecline_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected the bot to decline with     'I'm sorry' got Specify the category for the quiz. Please choose from the following: Geography, Science, or Art."
     ]
    }
   ],
   "source": [
    "evaluate_refusal(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    decline_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations in a CircleCI pipeline\n",
    "\n",
    "Put all these steps together into files to reuse later.\n",
    "\n",
    "**_Note:_** fixing the system_message by adding additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain_core.prompts           import ChatPromptTemplate\n",
    "from langchain_google_vertexai        import ChatVertexAI\n",
    "from langchain_core.output_parsers    import StrOutputParser\n",
    "\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "Additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "  Helper functions for writing the test cases\n",
    "\"\"\"\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message=system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "  \n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"user\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from langchain_core.prompts           import ChatPromptTemplate\n",
      "from langchain_google_vertexai        import ChatVertexAI\n",
      "from langchain_core.output_parsers    import StrOutputParser\n",
      "\n",
      "\n",
      "delimiter = \"####\"\n",
      "\n",
      "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
      "   Categories: Art, Science\n",
      "   Facts:\n",
      "    - Painted the Mona Lisa\n",
      "    - Studied zoology, anatomy, geology, optics\n",
      "    - Designed a flying machine\n",
      "  \n",
      "2. Subject: Paris\n",
      "   Categories: Art, Geography\n",
      "   Facts:\n",
      "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
      "    - Capital of France\n",
      "    - Most populous city in France\n",
      "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
      "\n",
      "3. Subject: Telescopes\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - Device to observe different objects\n",
      "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
      "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
      "\n",
      "4. Subject: Starry Night\n",
      "   Category: Art\n",
      "   Facts:\n",
      "    - Painted by Vincent van Gogh in 1889\n",
      "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
      "\n",
      "5. Subject: Physics\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - The sun doesn't change color during sunset.\n",
      "    - Water slows the speed of light\n",
      "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
      "\"\"\"\n",
      "\n",
      "system_message = f\"\"\"\n",
      "Follow these steps to generate a customized quiz for the user.\n",
      "The question will be delimited with four hashtags i.e {delimiter}\n",
      "\n",
      "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
      "should only refer to the category.\n",
      "\n",
      "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
      "* Geography\n",
      "* Science\n",
      "* Art\n",
      "\n",
      "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
      "\n",
      "{quiz_bank}\n",
      "\n",
      "Pick up to two subjects that fit the user's category. \n",
      "\n",
      "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
      "\n",
      "Use the following format for the quiz:\n",
      "Question 1:{delimiter} <question 1>\n",
      "\n",
      "Question 2:{delimiter} <question 2>\n",
      "\n",
      "Question 3:{delimiter} <question 3>\n",
      "\n",
      "Additional rules:\n",
      "\n",
      "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
      "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
      "\"\"\"\n",
      "\n",
      "\"\"\"\n",
      "  Helper functions for writing the test cases\n",
      "\"\"\"\n",
      "\n",
      "def assistant_chain(\n",
      "    system_message=system_message,\n",
      "    human_template=\"{question}\",\n",
      "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
      "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
      "                     temperature=0),\n",
      "    output_parser=StrOutputParser()):\n",
      "  \n",
      "  chat_prompt = ChatPromptTemplate.from_messages([\n",
      "      (\"system\", system_message),\n",
      "      (\"user\", human_template),\n",
      "  ])\n",
      "  return chat_prompt | llm | output_parser\n"
     ]
    }
   ],
   "source": [
    "!cat app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new file to include the evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_assistant.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_assistant.py\n",
    "from app import assistant_chain\n",
    "from app import system_message\n",
    "from langchain_core.prompts           import ChatPromptTemplate\n",
    "from langchain_google_vertexai        import ChatVertexAI\n",
    "from langchain_core.output_parsers    import StrOutputParser\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser)    \n",
    "  answer = assistant.invoke({\"question\": question})    \n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\"\n",
    "\n",
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
    "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
    "                     temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\"\n",
    "\n",
    "\"\"\"\n",
    "  Test cases\n",
    "\"\"\"\n",
    "\n",
    "def test_science_quiz():\n",
    "  \n",
    "  question  = \"Generate a quiz about science.\"\n",
    "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_geography_quiz():\n",
    "  question  = \"Generate a quiz about geography.\"\n",
    "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_refusal_rome():\n",
    "  question  = \"Help me create a quiz about Rome\"\n",
    "  decline_response = \"I'm sorry\"\n",
    "  evaluate_refusal(\n",
    "      system_message,\n",
    "      question,\n",
    "      decline_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from app import assistant_chain\n",
      "from app import system_message\n",
      "from langchain_core.prompts           import ChatPromptTemplate\n",
      "from langchain_google_vertexai        import ChatVertexAI\n",
      "from langchain_core.output_parsers    import StrOutputParser\n",
      "\n",
      "import os\n",
      "\n",
      "from dotenv import load_dotenv, find_dotenv\n",
      "_ = load_dotenv(find_dotenv())\n",
      "\n",
      "def eval_expected_words(\n",
      "    system_message,\n",
      "    question,\n",
      "    expected_words,\n",
      "    human_template=\"{question}\",\n",
      "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
      "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
      "                     temperature=0),\n",
      "    output_parser=StrOutputParser()):\n",
      "    \n",
      "  assistant = assistant_chain(\n",
      "      system_message,\n",
      "      human_template,\n",
      "      llm,\n",
      "      output_parser)    \n",
      "  answer = assistant.invoke({\"question\": question})    \n",
      "  print(answer)\n",
      "    \n",
      "  assert any(word in answer.lower() \\\n",
      "             for word in expected_words), \\\n",
      "    f\"Expected the assistant questions to include \\\n",
      "    '{expected_words}', but it did not\"\n",
      "\n",
      "def evaluate_refusal(\n",
      "    system_message,\n",
      "    question,\n",
      "    decline_response,\n",
      "    human_template=\"{question}\", \n",
      "    llm=ChatVertexAI(project='plucky-agent-412507', \n",
      "                     model_name=\"gemini-pro\", convert_system_message_to_human=True, \n",
      "                     temperature=0),\n",
      "    output_parser=StrOutputParser()):\n",
      "    \n",
      "  assistant = assistant_chain(human_template, \n",
      "                              system_message,\n",
      "                              llm,\n",
      "                              output_parser)\n",
      "  \n",
      "  answer = assistant.invoke({\"question\": question})\n",
      "  print(answer)\n",
      "  \n",
      "  assert decline_response.lower() in answer.lower(), \\\n",
      "    f\"Expected the bot to decline with \\\n",
      "    '{decline_response}' got {answer}\"\n",
      "\n",
      "\"\"\"\n",
      "  Test cases\n",
      "\"\"\"\n",
      "\n",
      "def test_science_quiz():\n",
      "  \n",
      "  question  = \"Generate a quiz about science.\"\n",
      "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
      "  eval_expected_words(\n",
      "      system_message,\n",
      "      question,\n",
      "      expected_subjects)\n",
      "\n",
      "def test_geography_quiz():\n",
      "  question  = \"Generate a quiz about geography.\"\n",
      "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
      "  eval_expected_words(\n",
      "      system_message,\n",
      "      question,\n",
      "      expected_subjects)\n",
      "\n",
      "def test_refusal_rome():\n",
      "  question  = \"Help me create a quiz about Rome\"\n",
      "  decline_response = \"I'm sorry\"\n",
      "  evaluate_refusal(\n",
      "      system_message,\n",
      "      question,\n",
      "      decline_response)\n"
     ]
    }
   ],
   "source": [
    "!cat test_assistant.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CircleCI config file\n",
    "Now let's set up our tests to run automatically in CircleCI.\n",
    "\n",
    "For this course, we've created a working CircleCI config file. Let's take a look at the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 2.1\n",
      "orbs:\n",
      "  # The python orb contains a set of prepackaged circleci configuration you can use repeatedly in your configurations files\n",
      "  # Orb commands and jobs help you with common scripting around a language/tool\n",
      "  # so you dont have to copy and paste it everywhere.\n",
      "  # See the orb documentation here: https://circleci.com/developer/orbs/orb/circleci/python\n",
      "  python: circleci/python@2.1.1\n",
      "\n",
      "parameters:\n",
      "  eval-mode:\n",
      "    type: string\n",
      "    default: \"commit\"\n",
      "\n",
      "workflows:\n",
      "  evaluate-commit:\n",
      "    when:\n",
      "      equal: [ commit, << pipeline.parameters.eval-mode >> ]\n",
      "    jobs:\n",
      "      - run-commit-evals:\n",
      "          context:\n",
      "            - dl-ai-courses\n",
      "  evaluate-release:\n",
      "    when:\n",
      "      equal: [ release, << pipeline.parameters.eval-mode >> ]\n",
      "    jobs:\n",
      "      - run-pre-release-evals:\n",
      "          context:\n",
      "            - dl-ai-courses\n",
      "  evaluate-all:\n",
      "    when:\n",
      "      equal: [ full, << pipeline.parameters.eval-mode >> ]\n",
      "    jobs:\n",
      "      - run-manual-evals:\n",
      "          context:\n",
      "            - dl-ai-courses\n",
      "  report-evals:\n",
      "    when:\n",
      "      equal: [ report, << pipeline.parameters.eval-mode >> ]\n",
      "    jobs:\n",
      "      - store-eval-artifacts:\n",
      "          context:\n",
      "            - dl-ai-courses\n",
      "\n",
      "jobs:\n",
      "  run-commit-evals:  # This is the name of the job, feel free to change it to better match what you're trying to do!\n",
      "    # These next lines defines a docker executors: https://circleci.com/docs/2.0/executor-types/\n",
      "    # You can specify an image from dockerhub or use one of the convenience images from CircleCI's Developer Hub\n",
      "    # A list of available CircleCI docker convenience images are available here: https://circleci.com/developer/images/image/cimg/python\n",
      "    # The executor is the environment in which the steps below will be executed - below will use a python 3.9 container\n",
      "    # Change the version below to your required version of python\n",
      "    docker:\n",
      "      - image: cimg/python:3.10.5\n",
      "    # Checkout the code as the first step. This is a dedicated CircleCI step.\n",
      "    # The python orb's install-packages step will install the dependencies from a Pipfile via Pipenv by default.\n",
      "    # Here we're making sure we use just use the system-wide pip. By default it uses the project root's requirements.txt.\n",
      "    # Then run your tests!\n",
      "    # CircleCI will report the results back to your VCS provider.\n",
      "    steps:\n",
      "      - checkout\n",
      "      - python/install-packages:\n",
      "          pkg-manager: pip\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
      "      - run:\n",
      "          name: Run assistant evals.\n",
      "          command: python -m pytest --junitxml results.xml test_assistant.py\n",
      "      - store_test_results:\n",
      "          path: results.xml\n",
      "  run-pre-release-evals:\n",
      "    docker:\n",
      "      - image: cimg/python:3.10.5\n",
      "    steps:\n",
      "      - checkout\n",
      "      - python/install-packages:\n",
      "          pkg-manager: pip\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
      "      - run:\n",
      "          name: Run release evals.\n",
      "          command: python -m pytest --junitxml results.xml test_release_evals.py\n",
      "      - store_test_results:\n",
      "          path: results.xml\n",
      "  run-manual-evals: \n",
      "    docker:\n",
      "      - image: cimg/python:3.10.5\n",
      "    steps:\n",
      "      - checkout\n",
      "      - python/install-packages:\n",
      "          pkg-manager: pip\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
      "      - run:\n",
      "          name: Run end to end evals.\n",
      "          command: python -m pytest --junitxml results.xml test_assistant.py test_release_evals.py\n",
      "      - store_test_results:\n",
      "          path: results.xml\n",
      "  store-eval-artifacts:\n",
      "    docker:\n",
      "      - image: cimg/python:3.10.5\n",
      "    steps:\n",
      "      - checkout\n",
      "      - python/install-packages:\n",
      "          pkg-manager: pip\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
      "      - run:\n",
      "          name: Save eval to html file\n",
      "          command: python save_eval_artifacts.py\n",
      "      - store_artifacts:\n",
      "          path: /tmp/eval_results.html\n",
      "          destination: eval_results.html"
     ]
    }
   ],
   "source": [
    "!cat circle_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the per-commit evals\n",
    "Push files into the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading test_assistant.py\n",
      "uploading app.py\n",
      "pushing files to: dl-cci-affirmative-pillow-49\n"
     ]
    }
   ],
   "source": [
    "from utils import push_files\n",
    "push_files(course_repo, course_branch, [\"app.py\", \"test_assistant.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the pipeline in CircleCI pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import trigger_commit_evals\n",
    "trigger_commit_evals(course_repo, course_branch, cci_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
